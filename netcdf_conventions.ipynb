{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d30c56e5-743a-4a75-96b3-c20abbdf163e",
   "metadata": {},
   "source": [
    "## Creating CF and ACDD compliant NetCDF file\n",
    "*Author:* Corrado Motta - corradomotta92@gmail.com\n",
    "\n",
    "This notebook aims to show how to add and possibly extract descriptive and administrative metadata from NetCDF files using python.\n",
    "With descriptive metadata we mean all metadata that relates to Discovery and Identification in the FAIR principles. They usually includes info such as _title_, _author_, _subjects_, _keywords_, _publisher_, and _urls_. Examples of standards are __DataCite__, __DublinCore__, __ISO 19115__. They are mainly domain agnostic. We also referred to them as _global metadata_ in the NetCDF context. On the other hand, administritive metadata are used to provide technical support for managing\n",
    "data in a dataset. Such metadata are domain specific. In the context of NetCDF, those are the attributes appended to each of our variables.\n",
    "\n",
    "Useful links:\n",
    "* [Notebook on FAIR and NetCDF](https://notebooks.githubusercontent.com/view/ipynb?color_mode=auto&commit=7438c171a8bfd838a97b9b859c8d92e0f9f01750&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f676973742f686576677972742f39663666613837383035643938636637386532356537373138616532336636622f7261772f373433386331373161386266643833386139376239623835396338643932653066396630313735302f6372656174655f4e65744344465f43465f414344442e6970796e62&logged_in=false&nwo=hevgyrt%2F9f6fa87805d98cf78e25e7718ae23f6b&path=create_NetCDF_CF_ACDD.ipynb&repository_id=99453764&repository_type=Gist)\n",
    "* [Attribute Convention for Data Discovery](https://wiki.esipfed.org/Attribute_Convention_for_Data_Discovery_1-3)\n",
    "* [CF Standard name table](https://cfconventions.org/Data/cf-standard-names/current/build/cf-standard-name-table.html)\n",
    "\n",
    "In this notebook we show how to make a FAIR-compliant NECTDF file from raw data. We use two conventions to reach this goal:\n",
    "* __CF__: Climate and Forecast convention. Mainly used for setting standard names to variables in NETCDF files and standard metadata for variables and dimensions.\n",
    "* __ACDD__: Attribute for Climate and Data Discovery. It can be used together with CF to populate the global attributes of a .nc file. Some opensource software already exist to read the global attributes and automatically generate Dublin Core or ISO 19115 descriptive metadata.\n",
    "\n",
    "Eventually, we also discuss how to generate/extract a XML file containg all the metadata following the ISO19115 standard schema. However, this part is not completed yet. For more information check the website pages.\n",
    "\n",
    "### Table of Contents\n",
    "* [1. Read configuration file](#read_cf)\n",
    "* [2. Import raw data](#import_data)\n",
    "* [3. Create NetCDF file with global metadata](#create_nc)\n",
    "* [4. Read a NetCDF file and print metadata](#read_nc)\n",
    "* [5. Export global metadata to ISO 19115 XML format](#export_iso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3501f77-73b1-4f40-b8dd-8ab2b03e3b56",
   "metadata": {},
   "source": [
    "### 1. Read configuration file <a class=\"anchor\" id=\"read_cf\"></a>\n",
    "First of all we import all the needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea9412aa-2880-4001-beec-157f02cb1796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add our own module to interact with the database\n",
    "import sys\n",
    "sys.path.append(\"fairdata\")\n",
    "\n",
    "# geopandas for plotting position\n",
    "import geopandas as gpd\n",
    "# to read netcdf file\n",
    "import netCDF4 as nc\n",
    "# numpy is used to work with n-dimensional arrays\n",
    "import numpy as np\n",
    "# os miscellaneous\n",
    "import os\n",
    "# work with table\n",
    "import pandas as pd\n",
    "# to make figures\n",
    "import matplotlib\n",
    "# to work on netcdf files\n",
    "import xarray\n",
    "# to read conf file\n",
    "import configparser\n",
    "# to work with path\n",
    "import ntpath\n",
    "# to add date\n",
    "from datetime import date\n",
    "# to generate ISO19115\n",
    "from bas_metadata_library.standards.iso_19115_2 import MetadataRecordConfigV2, MetadataRecord\n",
    "# To interact with the database\n",
    "from fairdata import metadataDB\n",
    "# for iso format\n",
    "from datetime import datetime\n",
    "# for plots\n",
    "import pandas_bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863fb8f9-7d79-4543-90f8-3d64f999eca8",
   "metadata": {},
   "source": [
    "Here we set the paths and the names of the file to analyze and the folder where to store results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2fc2cd0-f07b-462a-ac8d-7a90a00035c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath   = r\"data/20220915_ 81424_swamp2_zigzag_azimuth60_rpm1600_deltayaw60.csv\"\n",
    "resultPath = r\"demo_results\"\n",
    "confPath   = r\"conf/conf.ini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441acd7-e20f-4beb-b265-1f773cc801db",
   "metadata": {},
   "source": [
    "Then we check that the paths are good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b99a0d4-4625-404f-b4f6-2afc94cff856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to raw data: data/20220915_ 81424_swamp2_zigzag_azimuth60_rpm1600_deltayaw60.csv. Extracted filename: 20220915_ 81424_swamp2_zigzag_azimuth60_rpm1600_deltayaw60\n",
      "path to directory where to store data: demo_results\n"
     ]
    }
   ],
   "source": [
    "# Extract file name from the file path\n",
    "filename = ntpath.split(filePath)[1].split(\".\")[0]\n",
    "\n",
    "# print them \n",
    "if(filePath):\n",
    "    print(\"Path to raw data: {0}. Extracted filename: {1}\".format(filePath, filename))\n",
    "else:\n",
    "    print(\"Path not available\")\n",
    "if(resultPath):\n",
    "    print(\"path to directory where to store data: {0}\".format(resultPath))\n",
    "else:\n",
    "    print(\"Path not available\")\n",
    "    \n",
    "# check if result directory exists otherwise create it\n",
    "if(not os.path.exists(resultPath)):\n",
    "    os.makedirs(resultPath)\n",
    "    print(\"Created directory for storing results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7edacf-6eea-4dd8-b3ac-313a78a5b0c5",
   "metadata": {},
   "source": [
    "We are now ready to parse the configuration file which contains the global metadata. The configuration file can be manually filled using the _confTemplate.ini_ file or automatically generated from the interface that controls the vehicle (https://github.com/CorradoMotta/ASV_interface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "982efc9e-4ea1-491d-a72b-03e65c7e563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read generated file\n",
    "read_config = configparser.ConfigParser()\n",
    "read_config.read(confPath)\n",
    "my_complete_dict = dict(read_config.items('mandatory_global_attributes'))\n",
    "my_complete_dict = my_complete_dict | dict(read_config.items('optional_global_attributes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48bc0285-fb65-41e6-afdc-fd1cf1f8932e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following attributes will be added:\n",
      "1. keywords -> \"unmanned marine vehicles,marine robotics,autonomous systems\"\n",
      "2. institution -> CNR-INM\n",
      "3. platform -> SWAMP\n",
      "4. title -> Naval maneuver test in Venice\n",
      "5. conventions -> \"ACDD-1.3,CF-1.6\"\n",
      "6. license -> Creative Commons\n",
      "7. summary -> First test of naval maneuver in Venice. ZigZag and circle maneuvers were done using several azimuth and trhust values.\n",
      "8. creator_name -> \"Ferretti Roberta, Bibuli Marco, Motta Corrado\"\n",
      "9. product_version -> 1\n",
      "10. creator_email -> \"roberta.ferretti@cnr.it,marco.bibuli@cnr.it,corrado.motta@inm.cnr.it\"\n",
      "11. project -> INNOVAMARE\n",
      "12. processing_level -> raw data\n"
     ]
    }
   ],
   "source": [
    "print(\"The following attributes will be added:\")\n",
    "cont = 0\n",
    "key_list = []\n",
    "for key, value in my_complete_dict.items():\n",
    "    if(value):\n",
    "        cont +=1\n",
    "        my_complete_dict[key] = str(value).replace('\"','')\n",
    "        key_list.append(key)\n",
    "        print(str(cont) + \".\", key, '->', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c901f13d-4ee2-40d7-a176-19c82b469bdb",
   "metadata": {},
   "source": [
    "#### 1b. Checking global variables against JSON database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097d1d4-ce93-4e04-905d-58d60fd1f12d",
   "metadata": {},
   "source": [
    "Now we want to check if the attributes specifed in the configuration file satisfies the minimum set of mandatory global metadata stored our JSON database (check the \"database\" notebook for more information). All JSON database are stored in the database folder and have fixed names. We get access to the database entries by using the class _metadataDB_ in our own module. You can find info regarding our module in the gitHub pages. Also, in python, you can simply check what a method does using the help function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e04d9e-c77c-4b29-a0b0-e87db15cc74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function getById in module fairdata.metadataDB:\n",
      "\n",
      "getById(self, id)\n",
      "    Returns the database entry found for the required ID.\n",
      "    \n",
      "    Args:\n",
      "        id (str) : The database ID.\n",
      "    \n",
      "    Returns:\n",
      "        dict: The dictionary of the element. None if not present.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(metadataDB.metadataDB.getById)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f12ae489-a20e-4b99-8aae-e81d73eca382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database with name global_metadata.json already existing.                 All further operation will directly connect to it.\n",
      "title.. found\n",
      "summary.. found\n",
      "keywords.. found\n",
      "Conventions.. found\n",
      "creator_name.. found\n",
      "creator_email.. found\n",
      "institution.. found\n",
      "platform.. found\n",
      "license.. found\n",
      "product_version.. found\n"
     ]
    }
   ],
   "source": [
    "# Opening JSON file\n",
    "global_db = metadataDB.metadataDB('database/global_metadata.json')\n",
    "\n",
    "# Iterate over mandatory global metadata. When one is not present, stop and print it\n",
    "for key, value in global_db.getAll().items():\n",
    "    if(value['required'] and not value['auto']):\n",
    "        if(value['ACDD'].lower() in key_list):\n",
    "            print(value['ACDD'] + \".. found\")\n",
    "        else:\n",
    "            print(value['ACDD'], \"NOT found!\\n\\nPlease add a value for \",value['ACDD'])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1802d2-1e63-4745-8e2a-f78ee5910200",
   "metadata": {},
   "source": [
    "### 2. Import raw data <a class=\"anchor\" id=\"import_data\"></a>\n",
    "Let's import the telemetry data using read table method of pandas. We give as input the path, the delimiter as a single space, and the header list. As you can read in the github pages, we have two rows for the header, one for the log names and the second one for the standard names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0cf8d4b-af75-4241-91a5-42c8bb8684d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table(filePath, delimiter = ',', converters={('time', 'time'):str}, header=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab63a7b6-28e7-413b-a11e-25c934353575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81424.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81424.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81424.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81424.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81424.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>81521.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>81521.749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>81521.749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>81521.749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>81522.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          time\n",
       "0    81424.000\n",
       "1    81424.000\n",
       "2    81424.000\n",
       "3    81424.250\n",
       "4    81424.250\n",
       "..         ...\n",
       "577  81521.499\n",
       "578  81521.749\n",
       "579  81521.749\n",
       "580  81521.749\n",
       "581  81522.000\n",
       "\n",
       "[582 rows x 1 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the data\n",
    "data['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda64be2-1a00-4869-badd-ff7c28f0161b",
   "metadata": {},
   "source": [
    "Let's remove the second header and save the header tuples on a dedicated list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "474ff9d2-2387-46f0-b3bd-bc7d784303e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = data.columns\n",
    "\n",
    "# Remove header\n",
    "data = data.droplevel(1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbc058ee-cb9b-4376-923b-614362153d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('date', 'date')\n",
      "('time', 'time')\n",
      "('latitude', 'latitude')\n",
      "('longitude', 'longitude')\n",
      "('projection_x_coordinate', 'projection_x_coordinate')\n",
      "('projection_y_coordinate', 'projection_y_coordinate')\n",
      "('horizontalAccuracy', 'horizontalAccuracy')\n",
      "('llhPositionValidFlags', 'llhPositionValidFlags')\n",
      "('roll', 'platform_roll')\n",
      "('pitch', 'platform_pitch')\n",
      "('yaw', 'platform_yaw')\n",
      "('surge_velocity', 'platform_surge_rate_fore')\n",
      "('sway_velocity', 'platform_sway_rate_starboard')\n",
      "('heave_velocity', 'platform_heave_rate_down')\n",
      "('speedAccuracy', 'platform_speed_accuracy')\n",
      "('headingAccuracy', 'platform_track')\n",
      "('nedVelocityValidFlags', 'platform_speed_validity')\n",
      "('roll_rate', 'platform_roll_rate')\n",
      "('pitch_rate', 'platform_pitch_rate')\n",
      "('yaw_rate', 'platform_yaw_rate')\n",
      "('surge_acceleration', 'platform_surge_acceleration_fore')\n",
      "('sway_acceleration', 'platform_sway_acceleration_starboard')\n",
      "('heave_acceleration', 'platform_heave_acceleration_down')\n",
      "('azimuth_angle_reference', 'azimuth_angle_reference')\n",
      "('RL_azimuth_angle', 'azimuth_angle')\n",
      "('FL_azimuth_angle', 'azimuth_angle')\n",
      "('RR_azimuth_angle', 'azimuth_angle')\n",
      "('FR_azimuth_angle', 'azimuth_angle')\n",
      "('thruster_speed_reference', 'thruster_speed_reference')\n",
      "('RL_thruster_speed', 'thruster_speed')\n",
      "('FL_thruster_speed', 'thruster_speed')\n",
      "('RR_thruster_speed', 'thruster_speed')\n",
      "('FR_thruster_speed', 'thruster_speed')\n",
      "('RL_thruster_force', 'thruster_force')\n",
      "('FL_thruster_force', 'thruster_force')\n",
      "('RR_thruster_force', 'thruster_force')\n",
      "('FR_thruster_force', 'thruster_force')\n",
      "('RL_thruster_current', 'thruster_current')\n",
      "('FL_thruster_current', 'thruster_current')\n",
      "('RR_thruster_current', 'thruster_current')\n",
      "('FR_thruster_current', 'thruster_current')\n"
     ]
    }
   ],
   "source": [
    "for key in data_columns:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f5ed49-4f85-43e9-be13-5113a45800d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"datetime\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208dc40f-028e-45fa-9991-7e4477afd358",
   "metadata": {},
   "source": [
    "We are ready to add the global metadata. We want to follow the existing convention. It is important to have the datetime in **ISO 8601** format. In order to do so in an automated way, the date and time fields in the telemetry shall have a fixed format. Right now, in our telemetry, we have:\n",
    "\n",
    "- date : YYYYMMDD\n",
    "- time : HHMMSS.ms\n",
    "\n",
    "By knowing that, we can create a _datetime_ object in python, then we can convert it to ISO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f92c36-ca6f-43a5-9e68-aaa6196a10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join date and time\n",
    "data[\"datetime\"] = data[\"date\"].astype(str)+ \" \" + data[\"time\"].astype(str)\n",
    "# create a datetime python object\n",
    "data[\"datetime\"] = pd.to_datetime(data['datetime'], format='%yy%m%d %H%M%S.%s', infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dfb98e-c083-4f88-a802-ba0b66c74269",
   "metadata": {},
   "source": [
    "We are ready to add all global attributes to a data structure in python. So that we can use it later on to fill the NetCDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be6c2d5-abbd-416e-bdb0-ab382691ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_complete_dict[\"time_coverage_start\"] = data[\"datetime\"].min().isoformat()\n",
    "my_complete_dict[\"time_coverage_end\"] =   data[\"datetime\"].max().isoformat()\n",
    "my_complete_dict[\"geospatial_lat_max\"] =  data['latitude'].max()\n",
    "my_complete_dict[\"geospatial_lat_min\"] =  data['latitude'].min()\n",
    "my_complete_dict[\"geospatial_lat_units\"] = \"degree_north\"\n",
    "my_complete_dict[\"geospatial_lon_min\"] =  data['longitude'].min()\n",
    "my_complete_dict[\"geospatial_lon_max\"] =  data['longitude'].max()\n",
    "my_complete_dict[\"geospatial_lon_units\"] = \"degree_east\"\n",
    "my_complete_dict[\"date_created\"] = datetime.now().isoformat()\n",
    "my_complete_dict[\"time_coverage_duration\"] = (data[\"datetime\"].max() - data[\"datetime\"].min()).isoformat()\n",
    "my_complete_dict[\"time_coverage_resolution\"] = \"milliseconds\"\n",
    "\n",
    "# add to keylist\n",
    "key_list = []\n",
    "for key, value in my_complete_dict.items():\n",
    "    if(value):\n",
    "        key_list.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf7a28c-97a2-4be3-9294-7dcd797c979b",
   "metadata": {},
   "source": [
    "Check if any mandatory and auto global attributes are not filled yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b1a9b-6269-438b-b8dc-33196e86b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over mandatory global metadata. When one is not present, stop and print it\n",
    "for key, value in global_db.getAll().items():\n",
    "    if(value['required'] and value['auto']):\n",
    "        if(value['ACDD'].lower() in key_list):\n",
    "            print(value['ACDD'] + \".. found\")\n",
    "        else:\n",
    "            print(value['ACDD'], \"NOT found!\\n\\nPlease add a value for \",value['ACDD'])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea7a2a8-4079-406d-bb79-e246588304b4",
   "metadata": {},
   "source": [
    "Let's show on the map the trail of our vehicle. We use pandas bockeh functionality with openstreetmap as background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e16a3-b8c0-41ff-9736-b0411c39371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create geodataframe for maps\n",
    "gdf = gpd.GeoDataFrame(data, crs=\"EPSG:4326\", geometry=gpd.points_from_xy(data.longitude, data.latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4e11bc-f777-40b1-beb4-6f69179f39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bokeh\n",
    "pandas_bokeh.output_notebook()\n",
    "pd.set_option('plotting.backend', 'pandas_bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d4a73-a6d0-4f51-8bfc-26d232f63c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_plot = gdf.plot_bokeh(\n",
    "        title=filename,\n",
    "        tile_provider_url=r\"http://c.tile.openstreetmap.org/{Z}/{X}/{Y}.png\",\n",
    "        figsize=(900, 600),\n",
    "        line_color=\"black\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b164daf-15ad-49b1-8436-4e8d946b661c",
   "metadata": {},
   "source": [
    "We can also plot some data to check if our results look like expected. Let's print the variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d05d15e-e71a-4271-a67d-69642a2a06e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c27292-5344-42ad-8c23-24cabafe3073",
   "metadata": {},
   "source": [
    "We plot the thruster speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd74c84-3044-4a8d-99e3-d615a20a7325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.plot_bokeh(x='datetime', y=['FL_thruster_speed','FR_thruster_speed','RL_thruster_speed','RR_thruster_speed'], title = filename, figsize=(30,12))\n",
    "rpm_plot = data.plot(figsize=(1200, 600),x='datetime', y=['FL_thruster_speed','FR_thruster_speed','RL_thruster_speed','RR_thruster_speed'], title = filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb40ff6-ae6c-4670-b7aa-40a6e845775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the geometry field, which was added by bokeh\n",
    "if 'geometry' in data:\n",
    "    data.drop('geometry', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a778c09f-729f-4db0-8cf8-c00b9850774a",
   "metadata": {},
   "source": [
    "### 3. Create NetCDF file with global metadata <a class=\"anchor\" id=\"create_nc\"></a>\n",
    "Now that we know the data looks OK, we can create a NetCDF out of them. We use the _xarray_ package to do that. First we create an xarray object from the pandas data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46fb570-c4af-4215-9ce2-f22def255761",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr = xarray.Dataset.from_dataframe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2aa41f-be7f-4012-9aac-2d2be40d2b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00729c53-5a88-45ad-b1cf-f37cf1e7268d",
   "metadata": {},
   "source": [
    "By printing it, we can already see that now it has assumed the format of a NETCDF file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092db0aa-4290-4782-8cdb-fa72dbc3f1a2",
   "metadata": {},
   "source": [
    "It is time to add the __global__ metadata stored in our configuration file to the Netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd5123f-f617-4eaa-9e84-08ff496dc058",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in my_complete_dict.items():\n",
    "    if(value):\n",
    "        xr.attrs[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba5bb6-687b-4227-b3d0-762eabf515c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see the global attributes added!\n",
    "xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f214409-ac22-460b-8a11-4c06a59bfc16",
   "metadata": {},
   "source": [
    "Now, we can add the __attributes__ metadata to each variable to all variables that are found "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c863cbb0-2403-4fbf-83ed-f6bdd37abeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set json file\n",
    "\n",
    "# Opening JSON file\n",
    "variable_db = metadataDB.metadataDB('database/variable_metadata.json')\n",
    "\n",
    "# get all data\n",
    "variables = variable_db.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb76471b-a411-4b26-b253-314bba8de5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over each variable in the table and look for it in the database.\n",
    "for key in data_columns:\n",
    "    attr = variable_db.getEntry('long_name', key[1])\n",
    "    if(attr):\n",
    "        print(\"Attributes found for variable\", key[0])\n",
    "        for attr_name, value in attr[0].items():\n",
    "            if(attr_name!='version' and value):\n",
    "                xr[key[0]].attrs[attr_name] = value\n",
    "    else:\n",
    "        print(\"Attributes NOT found for variable\", key[0])\n",
    "print(\"\\nAll done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d93970-b47d-464e-9098-7b57ee9dc1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see the attributes on each variable now!\n",
    "xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237e3fa5-17a7-43e0-a7ac-0e875b78bc05",
   "metadata": {},
   "source": [
    "Now we can save it as a NETCDF with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ecabab-f6dd-449b-bfa8-2e56633423e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a result path for it\n",
    "result_path = os.path.join(resultPath, filename + \".nc\")\n",
    "\n",
    "# save to nc\n",
    "xr.to_netcdf(result_path)\n",
    "print(\"saved to {0}\".format(result_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e942ad98-ec86-426d-99df-fb8235c8e5b4",
   "metadata": {},
   "source": [
    "### 4. Read a NetCDF file and print metadata <a class=\"anchor\" id=\"read_nc\"></a>\n",
    "To read the NetCDF we use the homonym python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db0088-25b6-4e96-97af-8d8448df82fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = nc.Dataset(result_path)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81fc1d5-6985-442b-855d-a6237109434b",
   "metadata": {},
   "source": [
    "As an alternative we can use xarray as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c59791f-d1c3-4f9b-a708-b2517fafe23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_disk = xarray.open_dataset(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d706fe01-8dff-4bce-aa09-c0fc9b4671ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73710920-c4ce-4fba-bbcb-b68340216a86",
   "metadata": {},
   "source": [
    "Let's print the available global attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f27899-c60c-45a2-a095-f2a3ea33e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in ds_disk.attrs.items() :\n",
    "    print(key + \": \" + str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad4e61-fe75-45f9-b353-967ed13557b7",
   "metadata": {},
   "source": [
    "We can also define a simple function to read a single attribute on demand and one to return a variable object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6f670-9f8e-4d5c-a5b8-ade1926e7217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAttribute(myds, attribute):\n",
    "    # return the value of the attribute given as argument\n",
    "    \n",
    "    my_attr = None\n",
    "    try:\n",
    "         my_attr = getattr(myds, attribute)\n",
    "    except AttributeError as e: \n",
    "        print(\"arg <{0}> not present in the .nc file\".format(attribute))\n",
    "    return my_attr\n",
    "\n",
    "def getVariable(myds, variable):\n",
    "    # return the object of the variable given as argument\n",
    "    return myds.variables.get(variable, None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c5877-f830-42b8-8470-958f4d541f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geospatial_lat_max\n",
    "my_attr = \"summary\"\n",
    "print(getAttribute(ds, my_attr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77570c25-6333-44ac-8bdc-b7f0ef261a4a",
   "metadata": {},
   "source": [
    "Let's check all variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b81f9-10ca-4b61-992c-f051a206e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ds.variables.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526a9826-a80a-491a-b618-bf96365f93f9",
   "metadata": {},
   "source": [
    "And print a single variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c3a53d-61b7-4c07-9889-dcb36d10cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_var = \"FR_azimuth_angle\"\n",
    "obj_var = getVariable(ds, my_var)\n",
    "print(obj_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66afaa0d-61a1-4762-9dd0-2d1a4d47f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_var = \"sway_velocity\"\n",
    "obj_var = getVariable(ds, my_var)\n",
    "obj_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942fb9fe-7137-473d-8c80-e723a37043a7",
   "metadata": {},
   "source": [
    "### 4. Export global metadata to ISO 19115 XML format <a class=\"anchor\" id=\"export_iso\"></a>\n",
    "\n",
    "The metadata set in the conf.ini file are then added to the NETCDF4 using ACDD. However, we also want to be able to generate a ISO 199115 compliant metadata file, in the XML format. In fact, this is required by many different online repositories. \n",
    "\n",
    "Mapping between ACDD and ISO 19115 is provided [here](https://wiki.esipfed.org/Attribute_Convention_for_Data_Discovery_Mappings).\n",
    "\n",
    "__Note:__ this is under development. Right now, a python software packet named [bas-metadata-library](https://pypi.org/project/bas-metadata-library/) is used to generate the ISO file. This is done from the conf.ini list and not directly from the NETCDF4, which would be a more auspicable solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a34a2-7254-471d-ab67-149c06f19fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set contact\n",
    "if(my_complete_dict[\"creator_name\"]):\n",
    "    individual_contact = [{\"individual\": {\"name\": my_complete_dict[\"creator_name\"]}, \"role\": [\"originator\"]}]\n",
    "else:\n",
    "    print(\"not implemented yet\")\n",
    "    \n",
    "    \n",
    "minimal_record_config = {\n",
    "    \"hierarchy_level\": \"dataset\",\n",
    "    \"metadata\": {\n",
    "        \"language\": \"eng\",\n",
    "        \"character_set\": \"utf-8\",\n",
    "        \"contacts\": individual_contact,\n",
    "        \"date_stamp\": datetime.now(),\n",
    "    },\n",
    "    \"identification\": {\n",
    "        \"title\": {\"value\": my_complete_dict['title']},\n",
    "        \"dates\": {\"creation\": {\"date\": datetime.now(), \"date_precision\": \"year\"}},\n",
    "        \"abstract\": my_complete_dict['summary'],\n",
    "        \"keywords\":[{\"terms\":[{\"term\": item} for item in my_complete_dict['keywords'].split(\",\")]}],\n",
    "        \"character_set\": \"utf-8\",\n",
    "        \"language\": \"eng\",\n",
    "        \"topics\": [\"geoscientificInformation\"],\n",
    "        \"extent\": {\n",
    "            \"geographic\": {\n",
    "                \"bounding_box\": {\n",
    "                    \"west_longitude\": my_complete_dict[\"geospatial_lon_min\"],\n",
    "                    \"east_longitude\": my_complete_dict[\"geospatial_lon_max\"],\n",
    "                    \"south_latitude\": my_complete_dict[\"geospatial_lat_min\"],\n",
    "                    \"north_latitude\": my_complete_dict[\"geospatial_lat_max\"],\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "}\n",
    "configuration = MetadataRecordConfigV2(**minimal_record_config)\n",
    "record = MetadataRecord(configuration=configuration)\n",
    "document = record.generate_xml_document()\n",
    "\n",
    "# output document\n",
    "result_path = os.path.join(resultPath, filename + \"_metadata.xml\")\n",
    "# print(document.decode())\n",
    "f = open(result_path, \"w\")\n",
    "f.write(document.decode())\n",
    "f.close()\n",
    "print(\"metadata saved in\", result_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
